{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCQVdmhPjrkp4Ii4NlKbKI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivii-Jain/Text-Summarizer/blob/main/Text_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install nltk\n",
        "!pip install spacy"
      ],
      "metadata": {
        "id": "Oq2Spz6wOxAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sc0bDUE8OmUe"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import sys\n",
        "import math\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "import os\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Spacy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "xLgeXxGEO8KG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions for Reading Input Text"
      ],
      "metadata": {
        "id": "JMOU9JpePDGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Read .txt File and return its Text\n",
        "def file_text(filepath):\n",
        "    with open(filepath) as f:\n",
        "        text = f.read().replace(\"\\n\", '')\n",
        "        return text"
      ],
      "metadata": {
        "id": "Y_YYeXbkPAHu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Read PDF File and return its Text\n",
        "def pdfReader(pdf_path):\n",
        "    pdfReader = PdfReader(pdf_path)\n",
        "    count = len(pdfReader.pages)\n",
        "    print(\"\\nTotal Pages in pdf = \", count)\n",
        "\n",
        "    c = 'Y'\n",
        "    start_page = 0\n",
        "    end_page = count - 1\n",
        "    c = input(\"Do you want to read entire pdf ?[Y]/N  :  \")\n",
        "    if c == 'N' or c == 'n':\n",
        "        start_page = int(input(\"Enter start page number (Indexing starts from 0) :  \"))\n",
        "        end_page = int(input(f\"Enter end page number (Less than {count}) : \"))\n",
        "\n",
        "        if start_page < 0 or start_page >= count:\n",
        "            print(\"\\nInvalid Start page given\")\n",
        "            sys.exit()\n",
        "\n",
        "        if end_page < 0 or end_page >= count:\n",
        "            print(\"\\nInvalid End page given\")\n",
        "            sys.exit()\n",
        "\n",
        "    text = \"\"\n",
        "    for i in range(start_page, end_page + 1):\n",
        "        page = pdfReader.pages[i]\n",
        "        text += page.extract_text()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "d0Rqw5vuPHhp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Read Wikipedia page URL and return its Text\n",
        "def wiki_text(url):\n",
        "    scrap_data = urllib.request.urlopen(url)\n",
        "    article = scrap_data.read()\n",
        "    parsed_article = bs.BeautifulSoup(article, 'lxml')\n",
        "\n",
        "    paragraphs = parsed_article.find_all('p')\n",
        "    article_text = \"\"\n",
        "\n",
        "    for p in paragraphs:\n",
        "        article_text += p.text\n",
        "\n",
        "    # Removing all unwanted characters\n",
        "    article_text = re.sub(r'\\[[0-9]*\\]','', article_text)\n",
        "    return article_text"
      ],
      "metadata": {
        "id": "0fgVNWUiPNfD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Text"
      ],
      "metadata": {
        "id": "i6k5lMMZPWXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import spacy\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from google.colab import files\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def summarize_text(input_text_type, input_data=None):\n",
        "    # Step 1: Get text input\n",
        "    if input_text_type == 1:  # Typed input\n",
        "        text = input_data\n",
        "    elif input_text_type == 2:  # File input\n",
        "        print(\"Upload your file (.txt or .pdf)\")\n",
        "        uploaded = files.upload()\n",
        "        file_path = list(uploaded.keys())[0]\n",
        "        if file_path.endswith(\".txt\"):\n",
        "            text = file_text(file_path)\n",
        "        elif file_path.endswith(\".pdf\"):\n",
        "            text = pdfReader(file_path)\n",
        "        else:\n",
        "            print(\"Unsupported file type!\")\n",
        "            return None, None\n",
        "    elif input_text_type == 4:  # Wikipedia URL\n",
        "        text = wiki_text(input_data)\n",
        "    else:\n",
        "        print(\"Invalid input type!\")\n",
        "        return None, None\n",
        "\n",
        "    # Step 2: Preprocess text and split into sentences\n",
        "    original_words = [w for w in text.split() if w.isalnum()]\n",
        "    num_words_in_original_text = len(original_words)\n",
        "    text = nlp(text)\n",
        "    sentences = list(text.sents)\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    # Helper functions for processing\n",
        "    def frequency_matrix(sentences):\n",
        "        freq_matrix = {}\n",
        "        stopWords = nlp.Defaults.stop_words\n",
        "        for sent in sentences:\n",
        "            freq_table = {}\n",
        "            words = [word.text.lower() for word in sent if word.text.isalnum()]\n",
        "            for word in words:\n",
        "                word = lemmatizer.lemmatize(word)\n",
        "                if word not in stopWords:\n",
        "                    freq_table[word] = freq_table.get(word, 0) + 1\n",
        "            freq_matrix[sent[:15]] = freq_table\n",
        "        return freq_matrix\n",
        "\n",
        "    def tf_matrix(freq_matrix):\n",
        "        tf_matrix = {}\n",
        "        for sent, freq_table in freq_matrix.items():\n",
        "            tf_table = {word: count / len(freq_table) for word, count in freq_table.items()}\n",
        "            tf_matrix[sent] = tf_table\n",
        "        return tf_matrix\n",
        "\n",
        "    def sentences_per_words(freq_matrix):\n",
        "        sent_per_words = {}\n",
        "        for _, freq_table in freq_matrix.items():\n",
        "            for word in freq_table.keys():\n",
        "                sent_per_words[word] = sent_per_words.get(word, 0) + 1\n",
        "        return sent_per_words\n",
        "\n",
        "    def idf_matrix(freq_matrix, sent_per_words, total_sentences):\n",
        "        idf_matrix = {}\n",
        "        for sent, freq_table in freq_matrix.items():\n",
        "            idf_table = {word: math.log10(total_sentences / float(sent_per_words[word])) for word in freq_table.keys()}\n",
        "            idf_matrix[sent] = idf_table\n",
        "        return idf_matrix\n",
        "\n",
        "    def tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "        return {\n",
        "            sent: {word: tf_value * idf_matrix[sent][word] for word, tf_value in freq_table.items()}\n",
        "            for sent, freq_table in tf_matrix.items()\n",
        "        }\n",
        "\n",
        "    def score_sentences(tf_idf_matrix):\n",
        "        return {\n",
        "            sent: sum(scores.values()) / len(scores) if scores else 0\n",
        "            for sent, scores in tf_idf_matrix.items()\n",
        "        }\n",
        "\n",
        "    def average_score(sentence_scores):\n",
        "        return sum(sentence_scores.values()) / len(sentence_scores)\n",
        "\n",
        "    def create_summary(sentences, sentence_scores, threshold):\n",
        "        return \" \".join(\n",
        "            sentence.text for sentence in sentences\n",
        "            if sentence[:15] in sentence_scores and sentence_scores[sentence[:15]] >= threshold\n",
        "        )\n",
        "\n",
        "    # Step 3: Generate summary\n",
        "    freq_matrix_result = frequency_matrix(sentences)\n",
        "    tf_matrix_result = tf_matrix(freq_matrix_result)\n",
        "    sent_per_words_result = sentences_per_words(freq_matrix_result)\n",
        "    idf_matrix_result = idf_matrix(freq_matrix_result, sent_per_words_result, total_sentences)\n",
        "    tf_idf_matrix_result = tf_idf_matrix(tf_matrix_result, idf_matrix_result)\n",
        "    sentence_scores_result = score_sentences(tf_idf_matrix_result)\n",
        "    threshold = 1.3 * average_score(sentence_scores_result)\n",
        "    summary = create_summary(sentences, sentence_scores_result, threshold)\n",
        "\n",
        "    # Step 4: Display results\n",
        "    summary_stats = {\n",
        "        \"original_word_count\": num_words_in_original_text,\n",
        "        \"summary_word_count\": len(summary.split()),\n",
        "    }\n",
        "\n",
        "    print(\"\\n\\n\", \"*\" * 20, \"Summary\", \"*\" * 20, \"\\n\")\n",
        "    print(summary)\n",
        "    print(\"\\n\\n\", \"Original Words:\", summary_stats['original_word_count'],\n",
        "          \"| Summary Words:\", summary_stats['summary_word_count'])\n",
        "\n",
        "    return summary, summary_stats\n"
      ],
      "metadata": {
        "id": "TIO2K03WPbeW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwCcgPF_LhmS",
        "outputId": "0a8ead08-f900-4a8c-d23c-edc0935aa752"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarize from typed input**\n",
        "\n",
        "summary, stats = summarize_text(1, \"Your input text here\")\n",
        "\n",
        "**Summarize from a file**\n",
        "\n",
        "summary, stats = summarize_text(2)\n",
        "\n",
        "**Summarize from a Wikipedia URL**\n",
        "\n",
        "summary, stats = summarize_text(4,\"link\")"
      ],
      "metadata": {
        "id": "IU-LbU9wRm5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary,stats = summarize_text(1,\"Topic sentences are similar to mini thesis statements. Like a thesis statement, a topic sentence has a specific main point. Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph. Like the thesis statement, a topic sentence has a unifying function. But a thesis statement or topic sentence alone doesn’t guarantee unity. An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence. Note: Not all paragraphs need topic sentences. In particular, opening and closing paragraphs, which serve different functions from body paragraphs, generally don’t have topic sentences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0OqnMFTRffd",
        "outputId": "79c3187c-ea0d-4bd1-d32f-ae619863532d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " ******************** Summary ******************** \n",
            "\n",
            "An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence.\n",
            "\n",
            "\n",
            " Original Words: 95 | Summary Words: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary, stats = summarize_text(2)"
      ],
      "metadata": {
        "id": "a4h9kofUShCz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
